# ğŸ“š Paper-review

> A curated list of research papers I have read, along with summaries and key insights.

## ğŸ“Œ Papers by Category

### ğŸ·ï¸ LLM (Large Language Models)
#| Date | Title | Keywords | Affiliation | Notes | Link | Review |
#|------|-------|----------|-------------|-------|------|
#| 2024.04 | [OpenELM: An Efficient Language Model Family](https://arxiv.org/abs/XXXX) | Layer-wise Scaling | Apple | Summary | [ğŸ”—](https://arxiv.org/abs/XXXX) |
#| 2024.03 | [Jamba: A Hybrid Transformer-Mamba Language Model](https://arxiv.org/abs/XXXX) | Jamba, MoE | AI21 Labs | Summary | [ğŸ”—](https://arxiv.org/abs/XXXX) |

### ğŸ·ï¸ Reasoning & Inference
#| Date | Title | Keywords | Affiliation | Notes | Link | Review |
#|------|-------|----------|-------------|-------|------|
#| 2024.04 | [Self-Explore to Avoid the Pit](https://arxiv.org/abs/XXXX) | Self-Explore, Fine-Grained Learning Signals | KAIST | Summary | [ğŸ”—](https://arxiv.org/abs/XXXX) |
#| 2024.02 | [Boosting of Thoughts](https://arxiv.org/abs/XXXX) | Trial-and-Error | University of Toronto | Summary | [ğŸ”—](https://arxiv.org/abs/XXXX) |

### ğŸ·ï¸ Retrieval-Augmented Generation (RAG)
| Date | Title | Keywords | Affiliation | Notes | Link | Review |
|------|-------|----------|-------------|-------|------|--------|
| 2020.05 | [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) | RAG, Knowledge Retrieval | Facebook AI | Introduced RAG framework | [ğŸ”—](https://arxiv.org/abs/2005.11401) | [ğŸ“ Blog](https://velog.io/@ybonghy/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Retrieval-Augmented-Generation-for-Knowledge-Intensive-NLP-Tasks-2020.05)|

## ğŸ“ Paper Summary Format
- **[Paper Title]**(Paper Link)  
  - **ğŸ“… Date:** YYYY.MM  
  - **ğŸ·ï¸ Keywords:** Key concepts  
  - **ğŸ¢ Affiliation:** Institution  
  - **ğŸ” Summary:** A brief summary of the key findings  
  - **ğŸ“ Notes:** Additional references, blog posts, code implementations, etc.

---

âœ… **Updates:** Regularly adding new papers to keep track of recent research  
ğŸ“© **Contact:** Feel free to discuss papers via the `Issues` tab  
